import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from math import log

# Hamming weight function
def hw(x):
    return bin(x).count('1')

# AES S-box table
aes_sbox = [
    0x63, 0x7c, 0x77, 0x7b, 0xf2, 0x6b, 0x6f, 0xc5, 0x30, 0x01, 0x67, 0x2b, 0xfe, 0xd7, 0xab, 0x76,
    0xca, 0x82, 0xc9, 0x7d, 0xfa, 0x59, 0x47, 0xf0, 0xad, 0xd4, 0xa2, 0xaf, 0x9c, 0xa4, 0x72, 0xc0,
    0xb7, 0xfd, 0x93, 0x26, 0x36, 0x3f, 0xf7, 0xcc, 0x34, 0xa5, 0xe5, 0xf1, 0x71, 0xd8, 0x31, 0x15,
    0x04, 0xc7, 0x23, 0xc3, 0x18, 0x96, 0x05, 0x9a, 0x07, 0x12, 0x80, 0xe2, 0xeb, 0x27, 0xb2, 0x75,
    0x09, 0x83, 0x2c, 0x1a, 0x1b, 0x6e, 0x5a, 0xa0, 0x52, 0x3b, 0xd6, 0xb3, 0x29, 0xe3, 0x2f, 0x84,
    0x53, 0xd1, 0x00, 0xed, 0x20, 0xfc, 0xb1, 0x5b, 0x6a, 0xcb, 0xbe, 0x39, 0x4a, 0x4c, 0x58, 0xcf,
    0xd0, 0xef, 0xaa, 0xfb, 0x43, 0x4d, 0x33, 0x85, 0x45, 0xf9, 0x02, 0x7f, 0x50, 0x3c, 0x9f, 0xa8,
    0x51, 0xa3, 0x40, 0x8f, 0x92, 0x9d, 0x38, 0xf5, 0xbc, 0xb6, 0xda, 0x21, 0x10, 0xff, 0xf3, 0xd2,
    0xcd, 0x0c, 0x13, 0xec, 0x5f, 0x97, 0x44, 0x17, 0xc4, 0xa7, 0x7e, 0x3d, 0x64, 0x5d, 0x19, 0x73,
    0x60, 0x81, 0x4f, 0xdc, 0x22, 0x2a, 0x90, 0x88, 0x46, 0xee, 0xb8, 0x14, 0xde, 0x5e, 0x0b, 0xdb,
    0xe0, 0x32, 0x3a, 0x0a, 0x49, 0x06, 0x24, 0x5c, 0xc2, 0xd3, 0xac, 0x62, 0x91, 0x95, 0xe4, 0x79,
    0xe7, 0xc8, 0x37, 0x6d, 0x8d, 0xd5, 0x4e, 0xa9, 0x6c, 0x56, 0xf4, 0xea, 0x65, 0x7a, 0xae, 0x08,
    0xba, 0x78, 0x25, 0x2e, 0x1c, 0xa6, 0xb4, 0xc6, 0xe8, 0xdd, 0x74, 0x1f, 0x4b, 0xbd, 0x8b, 0x8a,
    0x70, 0x3e, 0xb5, 0x66, 0x48, 0x03, 0xf6, 0x0e, 0x61, 0x35, 0x57, 0xb9, 0x86, 0xc1, 0x1d, 0x9e,
    0xe1, 0xf8, 0x98, 0x11, 0x69, 0xd9, 0x8e, 0x94, 0x9b, 0x1e, 0x87, 0xe9, 0xce, 0x55, 0x28, 0xdf,
    0x8c, 0xa1, 0x89, 0x0d, 0xbf, 0xe6, 0x42, 0x68, 0x41, 0x99, 0x2d, 0x0f, 0xb0, 0x54, 0xbb, 0x16
]

# Parameters
key_byte = 0x2b  # Secret key byte (example)
num_traces = 5000  # Number of profiling traces
trace_length = 50  # Length of each trace
noise_std = 0.5  # Noise standard deviation

# Generate profiling dataset
plains = np.random.randint(0, 256, num_traces)
sensitives = np.array([aes_sbox[p ^ key_byte] for p in plains])
labels = sensitives  # Labels are the sensitive S-box outputs (256 classes)

traces = np.random.normal(0, 1, (num_traces, trace_length))
for i in range(num_traces):
    leak = hw(sensitives[i])  # Hamming weight leak
    traces[i, 10] = leak + np.random.normal(0, noise_std)  # Add leak at a fixed point with noise

# Convert to PyTorch tensors
X = torch.from_numpy(traces).float()
y = torch.from_numpy(labels).long()

# Simple train/test split (80/20)
split = int(0.8 * num_traces)
X_train, y_train = X[:split], y[:split]
X_test, y_test = X[split:], y[split:]

# DataLoader
train_ds = torch.utils.data.TensorDataset(X_train, y_train)
loader = torch.utils.data.DataLoader(train_ds, batch_size=32, shuffle=True)

# Simple MLP model
class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(trace_length, 128)
        self.fc2 = nn.Linear(128, 256)  # 256 output classes

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

model = Net()
optimizer = optim.Adam(model.parameters(), lr=0.001)  # Using Adam optimizer
loss_fn = nn.CrossEntropyLoss()

# Training loop
for epoch in range(10):
    model.train()
    for batch_x, batch_y in loader:
        out = model(batch_x)
        loss = loss_fn(out, batch_y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    print(f"Epoch {epoch+1}, Loss: {loss.item()}")

# Evaluation
model.eval()
with torch.no_grad():
    test_out = model(X_test)
    acc = (test_out.argmax(1) == y_test).float().mean().item()
print(f"Test Accuracy: {acc}")

# Attack phase (key recovery on new traces)
num_attack = 200  # Number of attack traces
plains_att = np.random.randint(0, 256, num_attack)
sens_att = np.array([aes_sbox[p ^ key_byte] for p in plains_att])
traces_att = np.random.normal(0, 1, (num_attack, trace_length))
for i in range(num_attack):
    leak = hw(sens_att[i])
    traces_att[i, 10] = leak + np.random.normal(0, noise_std)

X_att = torch.from_numpy(traces_att).float()
with torch.no_grad():
    preds = model(X_att)
probs = torch.softmax(preds, dim=1).numpy()

# Rank key guesses by log-likelihood
ranks = []
for g in range(256):
    logprob = 0.0
    for i, p in enumerate(plains_att):
        s = aes_sbox[p ^ g]
        logprob += log(probs[i, s] + 1e-10)
    ranks.append((logprob, g))

ranks.sort(reverse=True)
top_guesses = [hex(g) for _, g in ranks[:5]]
print(f"Top 5 key guesses: {top_guesses}")
print(f"Real key byte: {hex(key_byte)}")